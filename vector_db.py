"""
ç”µå•†å®¢æœå‘é‡æ•°æ®åº“æ¨¡å—
ä¸“é—¨è´Ÿè´£çŸ¥è¯†åº“çš„å‘é‡åŒ–å­˜å‚¨å’Œç›¸ä¼¼æ€§æ£€ç´¢
æ”¯æŒå¤šç§åµŒå…¥æ¨¡å‹
"""

import os
from typing import List, Dict, Any, Optional
from langchain_community.vectorstores import FAISS
from langchain_core.runnables import RunnablePassthrough

class EcommerceVectorDB:
    """ç”µå•†å®¢æœä¸“ç”¨å‘é‡æ•°æ®åº“"""
    
    def __init__(self):
        self.db = None
        self.retriever = None
        self.embeddings = None
        self.is_initialized = False
        self.provider = None
        
    def build_knowledge_base(self) -> tuple[List[str], List[Dict[str, str]]]:
        """æ„å»ºç”µå•†å®¢æœçŸ¥è¯†åº“"""
        print("ğŸ“š æ­£åœ¨æ„å»ºç”µå•†å®¢æœçŸ¥è¯†åº“...")
        
        knowledge_base = [
            {
                "question": "é€€è´§æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ",
                "answer": "1. åœ¨è®¢å•è¯¦æƒ…é¡µæäº¤é€€è´§ç”³è¯·\n2. é€‰æ‹©é€€è´§åŸå› å¹¶ä¸Šä¼ å•†å“ç…§ç‰‡\n3. ç­‰å¾…å®¢æœå®¡æ ¸ï¼ˆé€šå¸¸1-2ä¸ªå·¥ä½œæ—¥ï¼‰\n4. å®¡æ ¸é€šè¿‡åæŒ‰æç¤ºå¯„å›å•†å“\n5. æ”¶åˆ°é€€è´§å3ä¸ªå·¥ä½œæ—¥å†…é€€æ¬¾åˆ°è´¦"
            },
            {
                "question": "å•†å“ä¿ä¿®å¤šä¹…ï¼Ÿ",
                "answer": "æœ¬åº—æ‰€æœ‰å•†å“å‡æä¾›1å¹´å…¨å›½è”ä¿æœåŠ¡ã€‚è´­ä¹°åå‡­å‘ç¥¨å’Œä¿ä¿®å¡å¯åœ¨ä»»ä¸€æˆæƒç»´ä¿®ç‚¹äº«å—å…è´¹ç»´ä¿®æœåŠ¡ã€‚"
            },
            {
                "question": "æ”¯æŒå“ªäº›æ”¯ä»˜æ–¹å¼ï¼Ÿ",
                "answer": "æˆ‘ä»¬æ”¯æŒæ”¯ä»˜å®ã€å¾®ä¿¡æ”¯ä»˜ã€é“¶è¡Œå¡åœ¨çº¿æ”¯ä»˜ä»¥åŠè´§åˆ°ä»˜æ¬¾ç­‰å¤šç§æ”¯ä»˜æ–¹å¼ï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€è¦è‡ªç”±é€‰æ‹©ã€‚"
            },
            {
                "question": "é…é€æ—¶é—´éœ€è¦å¤šä¹…ï¼Ÿ",
                "answer": "ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œä¸‹å•å1-3ä¸ªå·¥ä½œæ—¥å‘è´§ï¼Œæ ¹æ®æ”¶è´§åœ°å€ä¸åŒï¼Œé…é€æ—¶é—´é€šå¸¸ä¸º2-7ä¸ªå·¥ä½œæ—¥ã€‚åè¿œåœ°åŒºå¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´ã€‚"
            },
            {
                "question": "å¦‚ä½•è”ç³»äººå·¥å®¢æœï¼Ÿ",
                "answer": "æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»äººå·¥å®¢æœï¼š\n- åœ¨çº¿å®¢æœï¼šç‚¹å‡»å³ä¸‹è§’'è”ç³»å®¢æœ'æŒ‰é’®\n- ç”µè¯å®¢æœï¼šæ‹¨æ‰“400-xxx-xxxx\n- å¾®ä¿¡å®¢æœï¼šæ·»åŠ å¾®ä¿¡å· xxx_service\nå·¥ä½œæ—¶é—´ï¼šæ¯å¤©9:00-21:00"
            },
            {
                "question": "å•†å“è´¨é‡é—®é¢˜æ€ä¹ˆåŠï¼Ÿ",
                "answer": "å¦‚æœæ‚¨æ”¶åˆ°çš„å•†å“å­˜åœ¨è´¨é‡é—®é¢˜ï¼š\n1. è¯·åœ¨ç­¾æ”¶æ—¶å½“åœºéªŒè´§\n2. å¦‚å‘ç°é—®é¢˜ç«‹å³æ‹ç…§ç•™å­˜è¯æ®\n3. è”ç³»å®¢æœç”³è¯·æ¢è´§æˆ–é€€æ¬¾\n4. æˆ‘ä»¬æ‰¿æ‹…å¾€è¿”è¿è´¹å¹¶ä¼˜å…ˆå¤„ç†"
            },
            {
                "question": "ä¼˜æƒ åˆ¸æ€ä¹ˆä½¿ç”¨ï¼Ÿ",
                "answer": "ä¼˜æƒ åˆ¸ä½¿ç”¨æ–¹æ³•ï¼š\n1. åœ¨è´­ç‰©è½¦é¡µé¢é€‰æ‹©å¯ç”¨ä¼˜æƒ åˆ¸\n2. ç¡®è®¤è®¢å•é‡‘é¢æ»¡è¶³ä½¿ç”¨æ¡ä»¶\n3. ç³»ç»Ÿè‡ªåŠ¨æŠµæ‰£ç›¸åº”é‡‘é¢\n4. æ³¨æ„æŸ¥çœ‹æœ‰æ•ˆæœŸå’Œä½¿ç”¨èŒƒå›´"
            },
            {
                "question": "å‘ç¥¨æ€ä¹ˆå¼€ï¼Ÿ",
                "answer": "å¼€ç¥¨æµç¨‹ï¼š\n1. ä¸‹å•æ—¶é€‰æ‹©'éœ€è¦å‘ç¥¨'\n2. å¡«å†™å‘ç¥¨æŠ¬å¤´å’Œç¨å·\n3. é€‰æ‹©ç”µå­å‘ç¥¨æˆ–çº¸è´¨å‘ç¥¨\n4. å‘è´§å3ä¸ªå·¥ä½œæ—¥å†…å¼€å…·å¹¶å‘é€"
            }
        ]
        
        # åˆ†ç¦»é—®é¢˜å’Œç­”æ¡ˆ
        texts = [item["answer"] for item in knowledge_base]
        metadatas = [{"question": item["question"], "answer": item["answer"]} for item in knowledge_base]
        
        print(f"âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼Œå…± {len(knowledge_base)} æ¡çŸ¥è¯†")
        return texts, metadatas
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        try:
            print("ğŸ“„ æ­£åœ¨åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")
            
            # æ„å»ºçŸ¥è¯†åº“
            texts, metadatas = self.build_knowledge_base()
            
            # è·å–åµŒå…¥æ¨¡å‹
            self.embeddings = self.get_embeddings_model()
            
            # åˆ›å»ºå‘é‡æ•°æ®åº“
            self.db = FAISS.from_texts(texts, self.embeddings, metadatas=metadatas)
            self.retriever = self.db.as_retriever(search_kwargs={"k": 1})
            
            self.is_initialized = True
            print(f"âœ… å‘é‡æ•°æ®åº“åˆå§‹åŒ–å®Œæˆï¼(ä½¿ç”¨ {self.provider} æ¨¡å‹)")
            return True
            
        except Exception as e:
            print(f"âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def search_similar(self, query: str, k: int = 1) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸ä¼¼å†…å®¹"""
        if not self.is_initialized:
            raise RuntimeError("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
        
        try:
            # æ‰§è¡Œç›¸ä¼¼æ€§æœç´¢
            results = self.db.similarity_search_with_score(query, k=k)
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            for doc, score in results:
                formatted_results.append({
                    "question": doc.metadata.get("question", ""),
                    "answer": doc.metadata.get("answer", doc.page_content),
                    "similarity_score": float(score),
                    "content": doc.page_content
                })
            
            return formatted_results
            
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []
    
    def get_retriever(self):
        """è·å–æ£€ç´¢å™¨"""
        if not self.is_initialized:
            raise RuntimeError("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
        return self.retriever
    
    def add_knowledge(self, question: str, answer: str):
        """åŠ¨æ€æ·»åŠ æ–°çŸ¥è¯†"""
        if not self.is_initialized:
            raise RuntimeError("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
        
        try:
            # æ·»åŠ æ–°æ–‡æœ¬åˆ°æ•°æ®åº“
            self.db.add_texts(
                texts=[answer],
                metadatas=[{"question": question, "answer": answer}]
            )
            print(f"âœ… æ–°çŸ¥è¯†å·²æ·»åŠ : {question}")
            
        except Exception as e:
            print(f"âŒ æ·»åŠ çŸ¥è¯†å¤±è´¥: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        if not self.is_initialized:
            return {"status": "not initialized"}
        
        try:
            # è·å–å‘é‡æ•°é‡ï¼ˆè¿‘ä¼¼ï¼‰
            vector_count = len(self.db.index_to_docstore_id) if hasattr(self.db, 'index_to_docstore_id') else 0
            
            return {
                "status": "initialized",
                "vector_count": vector_count,
                "model": self.provider,
                "search_top_k": 1
            }
        except Exception as e:
            return {"status": "error", "error": str(e)}


    def get_embeddings_model(self):
        """æ ¹æ®é…ç½®è·å–åˆé€‚çš„åµŒå…¥æ¨¡å‹"""
        dashscope_key = os.getenv('DASHSCOPE_API_KEY')
        openai_key = os.getenv('OPENAI_API_KEY')
        
        if dashscope_key:
            print("ğŸ”§ ä½¿ç”¨é€šä¹‰åƒé—®åµŒå…¥æ¨¡å‹...")
            try:
                from langchain_community.embeddings import DashScopeEmbeddings
                self.provider = "dashscope"
                return DashScopeEmbeddings(
                    dashscope_api_key=dashscope_key,
                    model="text-embedding-v1"
                )
            except ImportError:
                print("âš ï¸  é€šä¹‰åƒé—®åµŒå…¥æ¨¡å‹ä¸å¯ç”¨ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ...")
        
        if openai_key:
            print("ğŸ”§ ä½¿ç”¨OpenAIåµŒå…¥æ¨¡å‹...")
            try:
                from langchain_openai import OpenAIEmbeddings
                self.provider = "openai"
                return OpenAIEmbeddings(
                    timeout=60,
                    max_retries=3
                )
            except ImportError:
                print("âš ï¸  OpenAIåµŒå…¥æ¨¡å‹ä¸å¯ç”¨...")
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ç®€å•çš„æ–‡æœ¬å“ˆå¸Œ
        print("ğŸ”§ ä½¿ç”¨åŸºç¡€æ–‡æœ¬å¤„ç†...")
        self.provider = "basic"
        return BasicTextEmbeddings()


class BasicTextEmbeddings:
    """åŸºç¡€æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ä¸ºæ–‡æ¡£åˆ—è¡¨ç”ŸæˆåµŒå…¥å‘é‡"""
        return [self._simple_embed(text) for text in texts]
    
    def embed_query(self, text: str) -> List[float]:
        """ä¸ºæŸ¥è¯¢æ–‡æœ¬ç”ŸæˆåµŒå…¥å‘é‡"""
        return self._simple_embed(text)
    
    def _simple_embed(self, text: str) -> List[float]:
        """ç®€å•çš„æ–‡æœ¬å‘é‡åŒ–æ–¹æ³•"""
        # ä½¿ç”¨å­—ç¬¦é¢‘ç‡ä½œä¸ºç®€å•å‘é‡
        char_freq = {}
        for char in text.lower():
            if char.isalnum():
                char_freq[char] = char_freq.get(char, 0) + 1
        
        # åˆ›å»ºå›ºå®šé•¿åº¦å‘é‡
        vector = []
        chars = 'abcdefghijklmnopqrstuvwxyz0123456789'
        for char in chars:
            vector.append(char_freq.get(char, 0) / len(text) if text else 0)
        
        return vector
    
    def __call__(self, text: str) -> List[float]:
        """ä½¿å¯¹è±¡å¯è°ƒç”¨"""
        return self.embed_query(text)

# å…¨å±€å‘é‡æ•°æ®åº“å®ä¾‹
vector_db = EcommerceVectorDB()

if __name__ == "__main__":
    # æµ‹è¯•å‘é‡æ•°æ®åº“åŠŸèƒ½
    print("=== ç”µå•†å®¢æœå‘é‡æ•°æ®åº“æµ‹è¯• ===\n")
    
    # åˆå§‹åŒ–æ•°æ®åº“
    if vector_db.initialize():
        print("âœ… æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ\n")
        
        # æµ‹è¯•æœç´¢åŠŸèƒ½
        test_queries = [
            "æ€ä¹ˆé€€è´§ï¼Ÿ",
            "æ”¯æŒæ”¯ä»˜å®å—ï¼Ÿ",
            "å‘è´§è¦å‡ å¤©ï¼Ÿ"
        ]
        
        for query in test_queries:
            print(f"ğŸ” æœç´¢: '{query}'")
            results = vector_db.search_similar(query)
            
            if results:
                result = results[0]
                print(f"   ç›¸å…³é—®é¢˜: {result['question']}")
                print(f"   ç›¸ä¼¼åº¦: {result['similarity_score']:.3f}")
                print(f"   ç­”æ¡ˆé¢„è§ˆ: {result['answer'][:50]}...")
            else:
                print("   æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
            print()
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = vector_db.get_stats()
        print("ğŸ“Š æ•°æ®åº“ç»Ÿè®¡:")
        for key, value in stats.items():
            print(f"   {key}: {value}")
    else:
        print("âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥")